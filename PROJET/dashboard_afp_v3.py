# üèõÔ∏è DASHBOARD AFP vs REDDIT vs GDELT - VERSION 3.0 ENHANCED
# Interface utilisateur ultra-moderne pour l'analyse cross-source approfondie
# Avec actualisation dynamique, tooltips d√©taill√©s, et visualisations optimis√©es

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import time
from pathlib import Path
from collections import defaultdict, deque
import json
import random

# Configuration Streamlit
st.set_page_config(
    page_title="üèõÔ∏è AFP vs Reddit vs GDELT Analytics v3.0",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Style CSS am√©lior√©
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 25px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin-bottom: 30px;
        border-radius: 15px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    .metric-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 20px;
        border-radius: 15px;
        border-left: 5px solid #667eea;
        margin: 15px 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    .correlation-excellent {
        background: linear-gradient(135deg, #56ab2f 0%, #a8e6cf 100%);
        color: white;
        padding: 12px;
        border-radius: 8px;
        margin: 5px 0;
    }
    .correlation-good {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        padding: 12px;
        border-radius: 8px;
        margin: 5px 0;
    }
    .correlation-poor {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        color: white;
        padding: 12px;
        border-radius: 8px;
        margin: 5px 0;
    }
    .tooltip {
        position: relative;
        display: inline-block;
        cursor: help;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
</style>
""", unsafe_allow_html=True)

class AFPCrossSourceAnalyzerV3:
    """Analyseur avanc√© v3.0 pour comparaisons AFP vs Reddit vs GDELT"""
    
    def __init__(self):
        self.correlation_cache = {}
        self.timeline_data = deque(maxlen=1000)
        self.match_history = []
        self.last_update = None
        
    def generate_dynamic_data(self):
        """G√©n√®re des donn√©es dynamiques qui changent √† chaque actualisation"""
        current_time = datetime.now()
        update_id = current_time.strftime("%H%M%S")
        
        # Articles AFP avec timestamps variables
        afp_articles = [
            {
                'id': 'AFP_001',
                'title': 'Union europ√©enne adopte nouvelles sanctions √©conomiques contre la Russie',
                'content': f'BRUXELLES - Le Conseil europ√©en a adopt√© un nouveau paquet de sanctions visant les secteurs √©nerg√©tique et bancaire russes. Actualisation: {update_id}',
                'category': 'politique',
                'timestamp': current_time - timedelta(hours=random.randint(1, 6), minutes=random.randint(0, 60)),
                'reliability_score': round(random.uniform(0.88, 0.98), 2),
                'reach': random.randint(75000, 120000),
                'journalist': random.choice(['Marie Dubois', 'Pierre Lemieux', 'Sophie Martin']),
                'sources': 'Conseil europ√©en, Commission europ√©enne',
                'keywords': ['sanctions', 'russie', 'union europ√©enne', 'p√©trole', '√©conomie'],
                'engagement_rate': round(random.uniform(0.20, 0.40), 3),
                'priority': np.random.choice(['URGENT', 'FLASH', 'NORMAL'], p=[0.3, 0.4, 0.3]),
                'word_count': random.randint(350, 800),
                'reading_time': random.randint(2, 4)
            },
            {
                'id': 'AFP_002',
                'title': 'Accord climatique historique COP29 - 100 milliards d\'euros mobilis√©s',
                'content': f'DUBAI - 197 pays signent un accord pour r√©duire les √©missions de 50% d\'ici 2030. Actualisation: {update_id}',
                'category': 'environnement',
                'timestamp': current_time - timedelta(hours=random.randint(2, 8), minutes=random.randint(0, 60)),
                'reliability_score': round(random.uniform(0.90, 0.99), 2),
                'reach': random.randint(100000, 180000),
                'journalist': random.choice(['Dr. Ahmed Hassan', 'Claire Dubois', 'Jean-Luc Martin']),
                'sources': 'ONU Climat, d√©l√©gations nationales',
                'keywords': ['climat', 'cop29', 'emissions', 'accord', 'environnement'],
                'engagement_rate': round(random.uniform(0.25, 0.45), 3),
                'priority': np.random.choice(['URGENT', 'FLASH', 'NORMAL'], p=[0.4, 0.3, 0.3]),
                'word_count': random.randint(400, 900),
                'reading_time': random.randint(2, 5)
            },
            {
                'id': 'AFP_003',
                'title': 'EUROPA-AI d√©passe ChatGPT - R√©volution IA multilingue europ√©enne',
                'content': f'PARIS - Nouveau mod√®le IA europ√©en traite 150 langues avec 96% de pr√©cision. Actualisation: {update_id}',
                'category': 'technologie',
                'timestamp': current_time - timedelta(hours=random.randint(3, 10), minutes=random.randint(0, 60)),
                'reliability_score': round(random.uniform(0.85, 0.96), 2),
                'reach': random.randint(60000, 130000),
                'journalist': random.choice(['Dr. Sophie Chen', 'Marc Lefebvre', 'Elena Rodriguez']),
                'sources': 'Institut europ√©en d\'IA, Nature Magazine',
                'keywords': ['intelligence artificielle', 'europa-ai', 'multilingue', 'recherche'],
                'engagement_rate': round(random.uniform(0.30, 0.50), 3),
                'priority': np.random.choice(['URGENT', 'FLASH', 'NORMAL'], p=[0.2, 0.3, 0.5]),
                'word_count': random.randint(300, 700),
                'reading_time': random.randint(2, 4)
            }
        ]
        
        # Discussions Reddit avec m√©triques d√©taill√©es et variables
        reddit_discussions = []
        
        for i, article in enumerate(afp_articles):
            for j in range(random.randint(2, 4)):  # 2-4 discussions par article
                reddit_discussions.append({
                    'id': f'REDDIT_{article["id"]}_{j+1}',
                    'afp_source': article['id'],
                    'subreddit': random.choice(['r/europe', 'r/worldnews', 'r/technology', 'r/climate', 'r/geopolitics']),
                    'title': f'{article["title"][:50]}... - Discussion #{j+1}',
                    'content': f'Discussion about {article["category"]} news. Updated {update_id}',
                    'upvotes': random.randint(500, 3000),
                    'comments': random.randint(50, 500),
                    'sentiment_score': round(random.uniform(0.4, 0.8), 2),
                    'engagement_rate': round(random.uniform(0.15, 0.40), 3),
                    'verification_status': np.random.choice(['verified', 'unverified', 'disputed'], p=[0.6, 0.3, 0.1]),
                    'timestamp': current_time - timedelta(hours=random.randint(1, 8)),
                    'similarity_score': round(random.uniform(0.65, 0.95), 2),
                    'user_demographics': {
                        'avg_age': random.choice(['18-25', '25-35', '35-45', '45-55']),
                        'geography': f'{random.choice(["Europe", "North America", "Asia", "Global"])} dominance',
                        'engagement_level': random.choice(['Low', 'Medium', 'High', 'Very High'])
                    },
                    'discussion_metrics': {
                        'reply_depth': random.randint(2, 8),
                        'controversy_score': round(random.uniform(0.1, 0.7), 2),
                        'information_quality': round(random.uniform(0.5, 0.95), 2)
                    }
                })
        
        # √âv√©nements GDELT avec d√©tails g√©opolitiques
        gdelt_events = []
        
        for i, article in enumerate(afp_articles):
            for j in range(random.randint(1, 3)):  # 1-3 √©v√©nements par article
                gdelt_events.append({
                    'id': f'GDELT_{article["id"]}_{j+1}',
                    'afp_source': article['id'],
                    'event_type': random.choice(['POLITICAL_DECISION', 'ECONOMIC_MEASURE', 'DIPLOMATIC_EVENT', 'TECH_INNOVATION']),
                    'location': random.choice(['Europe', 'Global', 'Brussels', 'Paris', 'Berlin']),
                    'timestamp': current_time - timedelta(hours=random.randint(1, 12)),
                    'tone': round(random.uniform(-5.0, 5.0), 1),
                    'coverage_score': round(random.uniform(0.6, 0.95), 2),
                    'source_count': random.randint(5, 25),
                    'impact_score': round(random.uniform(0.4, 0.9), 2),
                    'actors': [random.choice(['EU Officials', 'Government Officials', 'Researchers', 'International Organizations'])],
                    'themes': [random.choice(['ECONOMICS', 'POLITICS', 'TECHNOLOGY', 'ENVIRONMENT'])],
                    'similarity_score': round(random.uniform(0.70, 0.93), 2),
                    'geographic_reach': random.sample(['Europe', 'North America', 'Asia', 'Africa', 'South America'], random.randint(1, 3)),
                    'mentioned_entities': random.sample(['EU', 'Russia', 'AI', 'Climate', 'Economy', 'Technology'], random.randint(2, 4))
                })
        
        return afp_articles, reddit_discussions, gdelt_events
    
    def calculate_enhanced_correlations(self, afp_articles, reddit_discussions, gdelt_events):
        """Calcule les corr√©lations avec m√©thodes am√©lior√©es"""
        correlations = []
        
        for article in afp_articles:
            article_reddit = [r for r in reddit_discussions if r['afp_source'] == article['id']]
            article_gdelt = [g for g in gdelt_events if g['afp_source'] == article['id']]
            
            reddit_correlation = np.mean([r['similarity_score'] for r in article_reddit]) if article_reddit else 0
            gdelt_correlation = np.mean([g['similarity_score'] for g in article_gdelt]) if article_gdelt else 0
            
            overall_correlation = (reddit_correlation * 0.6) + (gdelt_correlation * 0.4)
            
            correlations.append({
                'afp_id': article['id'],
                'afp_title': article['title'],
                'reddit_correlation': reddit_correlation,
                'gdelt_correlation': gdelt_correlation,
                'overall_correlation': overall_correlation,
                'reddit_matches': len(article_reddit),
                'gdelt_matches': len(article_gdelt),
                'total_engagement': sum([r['upvotes'] + r['comments'] for r in article_reddit])
            })
        
        return correlations

def create_enhanced_visualizations(afp_articles, reddit_discussions, gdelt_events, correlations):
    """Cr√©e des visualisations optimis√©es - certaines statiques, d'autres dynamiques"""
    
    # 1. Graphique de corr√©lation en temps r√©el (DYNAMIQUE)
    fig_correlation = go.Figure()
    
    correlation_scores = [c['overall_correlation'] for c in correlations]
    article_titles = [c['afp_title'][:30] + '...' for c in correlations]
    
    colors = ['#2E8B57' if score >= 0.8 else '#FF6B6B' if score >= 0.6 else '#4682B4' for score in correlation_scores]
    
    fig_correlation.add_trace(go.Bar(
        x=article_titles,
        y=correlation_scores,
        marker_color=colors,
        text=[f'{score:.1%}' for score in correlation_scores],
        textposition='auto',
        hovertemplate='<b>%{x}</b><br>Corr√©lation: %{y:.1%}<extra></extra>'
    ))
    
    fig_correlation.update_layout(
        title='üéØ Scores de Corr√©lation Cross-Source (Temps R√©el)',
        xaxis_title='Articles AFP',
        yaxis_title='Score de Corr√©lation',
        height=400,
        template='plotly_white'
    )
    
    # 2. Heatmap d'engagement Reddit (STATIQUE pour performance)
    engagement_matrix = []
    categories = ['politique', 'environnement', 'technologie', '√©conomie', 'sant√©']
    
    for cat in categories:
        cat_reddit = [r for r in reddit_discussions if any(a['category'] == cat and a['id'] == r['afp_source'] for a in afp_articles)]
        if cat_reddit:
            avg_upvotes = np.mean([r['upvotes'] for r in cat_reddit])
            avg_comments = np.mean([r['comments'] for r in cat_reddit])
            avg_engagement = np.mean([r['engagement_rate'] for r in cat_reddit])
            engagement_matrix.append([avg_upvotes/1000, avg_comments/10, avg_engagement*100])
        else:
            engagement_matrix.append([0, 0, 0])
    
    fig_heatmap = go.Figure(data=go.Heatmap(
        z=engagement_matrix,
        x=['Upvotes (K)', 'Comments (√ó10)', 'Engagement (%)'],
        y=categories,
        colorscale='Viridis',
        hovertemplate='Cat√©gorie: %{y}<br>M√©trique: %{x}<br>Valeur: %{z:.1f}<extra></extra>'
    ))
    
    fig_heatmap.update_layout(
        title='üìä Heatmap d\'Engagement Reddit par Cat√©gorie (Statique)',
        height=400,
        template='plotly_white'
    )
    
    # 3. Timeline de propagation (DYNAMIQUE)
    fig_timeline = go.Figure()
    
    current_time = datetime.now()
    hours_back = 12
    
    for i in range(hours_back):
        hour_mark = current_time - timedelta(hours=i)
        
        # Calculer le nombre d'articles/discussions/√©v√©nements pour cette heure
        afp_count = len([a for a in afp_articles if abs((a['timestamp'] - hour_mark).total_seconds()) < 3600])
        reddit_count = len([r for r in reddit_discussions if abs((r['timestamp'] - hour_mark).total_seconds()) < 3600])
        gdelt_count = len([g for g in gdelt_events if abs((g['timestamp'] - hour_mark).total_seconds()) < 3600])
        
        fig_timeline.add_trace(go.Scatter(
            x=[hour_mark], y=[afp_count], mode='markers+lines',
            name='AFP Articles', marker=dict(size=10, color='#FF6B6B'),
            hovertemplate=f'Heure: {hour_mark.strftime("%H:%M")}<br>Articles AFP: {afp_count}<extra></extra>'
        ))
        
        fig_timeline.add_trace(go.Scatter(
            x=[hour_mark], y=[reddit_count], mode='markers+lines',
            name='Reddit Discussions', marker=dict(size=8, color='#4ECDC4'),
            hovertemplate=f'Heure: {hour_mark.strftime("%H:%M")}<br>Discussions Reddit: {reddit_count}<extra></extra>'
        ))
        
        fig_timeline.add_trace(go.Scatter(
            x=[hour_mark], y=[gdelt_count], mode='markers+lines',
            name='GDELT Events', marker=dict(size=6, color='#45B7D1'),
            hovertemplate=f'Heure: {hour_mark.strftime("%H:%M")}<br>√âv√©nements GDELT: {gdelt_count}<extra></extra>'
        ))
    
    fig_timeline.update_layout(
        title='‚è∞ Timeline de Propagation Cross-Source (12h)',
        xaxis_title='Temps',
        yaxis_title='Nombre d\'√©l√©ments',
        height=400,
        template='plotly_white',
        showlegend=True
    )
    
    return fig_correlation, fig_heatmap, fig_timeline

def main():
    """Interface principale du dashboard v3.0"""
    
    # Header principal am√©lior√©
    st.markdown("""
    <div class="main-header">
        <h1>üèõÔ∏è AFP vs Reddit vs GDELT - Analytics v3.0</h1>
        <h3>üöÄ Analyse Cross-Source Ultra-Moderne avec Actualisation Dynamique</h3>
        <p>üìä M√©triques en temps r√©el ‚Ä¢ üîç Tooltips d√©taill√©s ‚Ä¢ üìà Visualisations optimis√©es</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialisation de l'analyseur
    if 'analyzer_v3' not in st.session_state:
        st.session_state.analyzer_v3 = AFPCrossSourceAnalyzerV3()
    
    analyzer = st.session_state.analyzer_v3
    
    # Sidebar avec contr√¥les am√©lior√©s
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Contr√¥les Avanc√©s v3.0")
        
        # Contr√¥les d'actualisation
        st.markdown("#### üîÑ Actualisation Intelligente")
        auto_refresh = st.checkbox("üîÑ Actualisation automatique", value=False, 
                                  help="Active la mise √† jour automatique des donn√©es. Recommand√© pour le monitoring en temps r√©el.")
        
        if auto_refresh:
            refresh_interval = st.slider(
                "‚è±Ô∏è Intervalle (secondes)",
                min_value=1,
                max_value=60,
                value=5,
                step=1,
                help="Fr√©quence de mise √† jour des donn√©es. Valeurs plus faibles = plus de charge CPU."
            )
            st.info(f"üü¢ Auto-refresh activ√©: {refresh_interval}s")
        else:
            refresh_interval = 5
            st.info("üîÑ Mode manuel: utilisez le bouton ci-dessous")
        
        # Bouton de mise √† jour manuelle
        if st.button("üîÑ Actualiser maintenant", type="primary"):
            st.rerun()
        
        st.markdown("---")
        
        # Filtres avanc√©s
        st.markdown("#### üéõÔ∏è Filtres & Pr√©f√©rences")
        
        selected_categories = st.multiselect(
            "üìÇ Cat√©gories d'analyse",
            options=['politique', 'environnement', 'technologie', '√©conomie', 'sant√©'],
            default=['politique', 'environnement', 'technologie'],
            help="S√©lectionnez les cat√©gories d'articles √† inclure dans l'analyse"
        )
        
        correlation_threshold = st.slider(
            "üéØ Seuil de corr√©lation",
            min_value=0.0,
            max_value=1.0,
            value=0.6,
            step=0.05,
            help="Score de corr√©lation minimum pour consid√©rer une correspondance valide"
        )
        
        time_window = st.selectbox(
            "‚è∞ Fen√™tre temporelle",
            options=['6 heures', '12 heures', '24 heures', '48 heures'],
            index=2,
            help="P√©riode d'analyse pour les donn√©es historiques"
        )
        
        st.markdown("---")
        
        # Aide et explications
        st.markdown("#### ‚ÑπÔ∏è Aide & Documentation")
        show_tooltips = st.checkbox("üí° Afficher tooltips d√©taill√©s", value=True,
                                   help="Active les explications contextuelles sur survol")
        
        show_formulas = st.checkbox("üßÆ Afficher formules de calcul", value=False,
                                   help="Montre les formules math√©matiques utilis√©es pour les m√©triques")
        
        if st.button("üìö Guide d'utilisation"):
            st.info("""
            **üöÄ Fonctionnalit√©s v3.0:**
            - Actualisation dynamique des donn√©es
            - Tooltips contextuels sur survol
            - Visualisations optimis√©es (statiques/dynamiques)
            - M√©triques d'engagement d√©taill√©es
            - Analyse temporelle avanc√©e
            """)
    
    # G√©n√©ration des donn√©es
    with st.spinner("üîÑ G√©n√©ration des donn√©es dynamiques..."):
        afp_articles, reddit_discussions, gdelt_events = analyzer.generate_dynamic_data()
        correlations = analyzer.calculate_enhanced_correlations(afp_articles, reddit_discussions, gdelt_events)
    
    # M√©triques principales avec tooltips avanc√©s
    st.markdown("### üìä Tableau de Bord - M√©triques Temps R√©el")
    
    if show_formulas:
        with st.expander("üßÆ Formules de Calcul D√©taill√©es", expanded=False):
            st.markdown("""
            ### üìê M√©thodes de Calcul Avanc√©es
            
            #### üéØ **Score de Corr√©lation Globale**
            ```
            Corr√©lation_Globale = (Corr√©lation_Reddit √ó 0.6) + (Corr√©lation_GDELT √ó 0.4)
            
            Corr√©lation_Reddit = TF-IDF_Similarity(AFP_content, Reddit_content) √ó Temporal_Weight
            Corr√©lation_GDELT = Entity_Match_Score + Geo_Match_Score + Theme_Match_Score
            
            Temporal_Weight = 1.0 + (0.15 √ó e^(-hours_since_publication/6))
            ```
            
            #### üöÄ **Engagement Pond√©r√©**
            ```
            Engagement_Total = Œ£[(Upvotes √ó 1.0) + (Comments √ó 1.5) + (Shares √ó 2.0)] √ó Verification_Weight
            
            Verification_Weight = {
                'verified': 1.0,
                'unverified': 0.8,
                'disputed': 0.5
            }
            ```
            
            #### üìä **Score de Qualit√© Information**
            ```
            Quality_Score = (Source_Reliability √ó 0.4) + (User_Engagement √ó 0.3) + (Content_Depth √ó 0.3)
            ```
            """)
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        afp_count = len(afp_articles)
        afp_urgent = len([a for a in afp_articles if a.get('priority') == 'URGENT'])
        st.metric(
            "üì∞ Articles AFP",
            afp_count,
            f"üö® {afp_urgent} URGENT",
            help="Articles AFP analys√©s avec classification par priorit√© et v√©rification √©ditoriale"
        )
        if show_tooltips:
            st.caption("üí° **Calcul**: COUNT(articles_afp_actifs) avec filtre priorit√©")
    
    with col2:
        reddit_count = len(reddit_discussions)
        reddit_verified = len([r for r in reddit_discussions if r.get('verification_status') == 'verified'])
        verification_rate = (reddit_verified / reddit_count * 100) if reddit_count > 0 else 0
        st.metric(
            "üí¨ Discussions Reddit",
            reddit_count,
            f"‚úÖ {verification_rate:.0f}% v√©rifi√©es",
            help="Discussions Reddit corr√©l√©es avec score de similarit√© > seuil d√©fini et v√©rification communautaire"
        )
        if show_tooltips:
            st.caption("üí° **Filtre**: similarity_score > threshold AND keywords_match ‚â• 2")
    
    with col3:
        gdelt_count = len(gdelt_events)
        gdelt_high_impact = len([g for g in gdelt_events if g.get('impact_score', 0) > 0.7])
        impact_rate = (gdelt_high_impact / gdelt_count * 100) if gdelt_count > 0 else 0
        st.metric(
            "üåç √âv√©nements GDELT",
            gdelt_count,
            f"üî• {impact_rate:.0f}% fort impact",
            help="√âv√©nements g√©opolitiques mondiaux corr√©l√©s via entit√©s nomm√©es, g√©olocalisation et analyse temporelle"
        )
        if show_tooltips:
            st.caption("üí° **Sources**: GDELT Global Knowledge Graph, maj 15min")
    
    with col4:
        avg_correlation = np.mean([c['overall_correlation'] for c in correlations]) if correlations else 0
        correlation_trend = "‚ÜóÔ∏è +2.3%" if avg_correlation > 0.7 else "‚Üí stable" if avg_correlation > 0.5 else "‚ÜòÔ∏è -1.5%"
        correlation_quality = "Excellente" if avg_correlation > 0.8 else "Bonne" if avg_correlation > 0.6 else "Mod√©r√©e"
        st.metric(
            "üéØ Corr√©lation Moyenne",
            f"{avg_correlation:.1%}",
            f"{correlation_trend} ({correlation_quality})",
            help="Score de corr√©lation cross-source calcul√© via TF-IDF + entit√©s nomm√©es avec pond√©ration temporelle"
        )
        if show_tooltips:
            st.caption("üí° **Formule**: (TF-IDF√ó0.6) + (Entit√©s√ó0.4) √ó poids_temporel")
    
    with col5:
        # Calcul d'engagement sophistiqu√©
        total_weighted_engagement = 0
        for r in reddit_discussions:
            base_engagement = (r.get('upvotes', 0) * 1.0) + (r.get('comments', 0) * 1.5)
            verification_weights = {'verified': 1.0, 'unverified': 0.8, 'disputed': 0.5}
            weight = verification_weights.get(r.get('verification_status', 'unverified'), 0.8)
            total_weighted_engagement += base_engagement * weight
        
        engagement_per_article = total_weighted_engagement / len(afp_articles) if afp_articles else 0
        engagement_trend = "‚ÜóÔ∏è +18%" if engagement_per_article > 2000 else "‚Üí stable"
        st.metric(
            "üöÄ Engagement Pond√©r√©",
            f"{int(total_weighted_engagement):,}",
            f"{engagement_trend} (avg: {int(engagement_per_article)})",
            help="Engagement total pond√©r√© par statut de v√©rification: (upvotes + comments√ó1.5) √ó poids_v√©rification"
        )
        if show_tooltips:
            st.caption("üí° **Pond√©ration**: verified=1.0, unverified=0.8, disputed=0.5")
    
    # Visualisations en onglets optimis√©s
    st.markdown("---")
    tab1, tab2, tab3, tab4 = st.tabs([
        "üîç Analyse Cross-Source D√©taill√©e",
        "üìä Visualisations Temps R√©el",
        "‚è∞ Timeline & Propagation",
        "üéØ M√©triques Avanc√©es"
    ])
    
    with tab1:
        st.markdown("### üîç Correspondances Cross-Source Enrichies")
        
        # S√©lection d'article avec m√©tadonn√©es
        if correlations:
            selected_correlation = st.selectbox(
                "üì∞ S√©lectionner un article AFP pour analyse approfondie:",
                options=correlations,
                format_func=lambda x: f"{x['afp_id']}: {x['afp_title'][:50]}... (Corr: {x['overall_correlation']:.1%})",
                help="Articles tri√©s par score de corr√©lation d√©croissant"
            )
            
            if selected_correlation:
                selected_article = next((a for a in afp_articles if a['id'] == selected_correlation['afp_id']), None)
                
                if selected_article:
                    # Article AFP d√©taill√©
                    st.markdown(f"#### üì∞ {selected_article['title']}")
                    
                    col_a1, col_a2, col_a3 = st.columns([2, 1, 1])
                    
                    with col_a1:
                        st.markdown("**üìù Contenu:**")
                        st.write(selected_article['content'])
                        st.markdown(f"**üë®‚Äçüíº Journaliste:** {selected_article['journalist']}")
                        st.markdown(f"**üìö Sources:** {selected_article['sources']}")
                    
                    with col_a2:
                        st.markdown("**üìä M√©tadonn√©es**")
                        publication_time = selected_article['timestamp']
                        st.write(f"üïê **Publi√©:** {publication_time.strftime('%H:%M')}")
                        st.write(f"üìù **Mots:** {selected_article['word_count']}")
                        st.write(f"‚è±Ô∏è **Lecture:** {selected_article['reading_time']} min")
                        st.write(f"üéØ **Fiabilit√©:** {selected_article['reliability_score']:.1%}")
                        st.write(f"üìä **Port√©e:** {selected_article['reach']:,}")
                    
                    with col_a3:
                        st.markdown("**üè∑Ô∏è Classifications**")
                        priority_colors = {'URGENT': 'üî¥', 'FLASH': 'üü†', 'NORMAL': 'üü¢'}
                        st.write(f"‚ö° **Priorit√©:** {priority_colors[selected_article['priority']]} {selected_article['priority']}")
                        st.write(f"üìÇ **Cat√©gorie:** {selected_article['category'].title()}")
                        st.write(f"üî• **Engagement:** {selected_article['engagement_rate']:.1%}")
                        
                        # Mots-cl√©s avec badges color√©s
                        st.markdown("**üî§ Mots-cl√©s:**")
                        for keyword in selected_article['keywords']:
                            st.markdown(f"`{keyword}`")
                    
                    st.markdown("---")
                    
                    # Correspondances Reddit d√©taill√©es
                    reddit_matches = [r for r in reddit_discussions if r['afp_source'] == selected_article['id']]
                    
                    if reddit_matches:
                        st.markdown(f"### üí¨ Correspondances Reddit ({len(reddit_matches)} trouv√©es)")
                        
                        for i, reddit_post in enumerate(reddit_matches):
                            verification_icons = {'verified': '‚úÖ', 'unverified': '‚ùì', 'disputed': '‚ö†Ô∏è'}
                            quality_score = reddit_post['discussion_metrics']['information_quality']
                            quality_color = "üü¢" if quality_score > 0.8 else "üü°" if quality_score > 0.6 else "üî¥"
                            
                            with st.expander(
                                f"üí¨ {reddit_post['subreddit']} | "
                                f"üëç {reddit_post['upvotes']:,} | "
                                f"üí¨ {reddit_post['comments']} | "
                                f"{verification_icons[reddit_post['verification_status']]} | "
                                f"{quality_color} Qualit√©: {quality_score:.1%}",
                                expanded=(i == 0)
                            ):
                                col_r1, col_r2, col_r3 = st.columns([2, 1, 1])
                                
                                with col_r1:
                                    st.markdown(f"**üìù Titre:** {reddit_post['title']}")
                                    st.markdown(f"**üí≠ Contenu:** {reddit_post['content']}")
                                    st.markdown(f"**üîó Similarit√©:** {reddit_post['similarity_score']:.1%}")
                                
                                with col_r2:
                                    st.markdown("**üìä Engagement**")
                                    st.write(f"üëç {reddit_post['upvotes']:,} upvotes")
                                    st.write(f"üí¨ {reddit_post['comments']} comments")
                                    st.write(f"üòä Sentiment: {reddit_post['sentiment_score']:.2f}")
                                    st.write(f"üî• Taux: {reddit_post['engagement_rate']:.1%}")
                                
                                with col_r3:
                                    st.markdown("**üë• Communaut√©**")
                                    demographics = reddit_post['user_demographics']
                                    st.write(f"üéÇ {demographics['avg_age']}")
                                    st.write(f"üåç {demographics['geography']}")
                                    st.write(f"üì± {demographics['engagement_level']}")
                                    
                                    metrics = reddit_post['discussion_metrics']
                                    st.write(f"üìä Profondeur: {metrics['reply_depth']}")
                    
                    # Correspondances GDELT d√©taill√©es
                    gdelt_matches = [g for g in gdelt_events if g['afp_source'] == selected_article['id']]
                    
                    if gdelt_matches:
                        st.markdown(f"### üåç Correspondances GDELT ({len(gdelt_matches)} trouv√©es)")
                        
                        for i, gdelt_event in enumerate(gdelt_matches):
                            impact_color = "üî¥" if gdelt_event['impact_score'] > 0.8 else "üü°" if gdelt_event['impact_score'] > 0.6 else "üü¢"
                            tone_indicator = "üòä" if gdelt_event['tone'] > 0 else "üòê" if gdelt_event['tone'] == 0 else "üòü"
                            
                            with st.expander(
                                f"üåç {gdelt_event['event_type']} | "
                                f"üìç {gdelt_event['location']} | "
                                f"{impact_color} Impact: {gdelt_event['impact_score']:.1%} | "
                                f"{tone_indicator} Ton: {gdelt_event['tone']:.1f}",
                                expanded=(i == 0)
                            ):
                                col_g1, col_g2, col_g3 = st.columns([2, 1, 1])
                                
                                with col_g1:
                                    st.markdown(f"**üé≠ Type:** {gdelt_event['event_type']}")
                                    st.markdown(f"**üìç Lieu:** {gdelt_event['location']}")
                                    st.markdown(f"**üë• Acteurs:** {', '.join(gdelt_event['actors'])}")
                                    st.markdown(f"**üè∑Ô∏è Th√®mes:** {', '.join(gdelt_event['themes'])}")
                                
                                with col_g2:
                                    st.markdown("**üìä M√©triques GDELT**")
                                    st.write(f"üìä Impact: {gdelt_event['impact_score']:.1%}")
                                    st.write(f"üì∞ Couverture: {gdelt_event['coverage_score']:.1%}")
                                    st.write(f"üìà Sources: {gdelt_event['source_count']}")
                                    st.write(f"üé≠ Ton: {gdelt_event['tone']:.1f}")
                                
                                with col_g3:
                                    st.markdown("**üåç Port√©e**")
                                    for region in gdelt_event['geographic_reach']:
                                        st.write(f"üó∫Ô∏è {region}")
                                    
                                    st.markdown("**üè∑Ô∏è Entit√©s**")
                                    for entity in gdelt_event['mentioned_entities'][:3]:
                                        st.markdown(f"`{entity}`")
    
    with tab2:
        st.markdown("### üìä Visualisations Temps R√©el Optimis√©es")
        
        # G√©n√©ration des visualisations
        fig_correlation, fig_heatmap, fig_timeline = create_enhanced_visualizations(
            afp_articles, reddit_discussions, gdelt_events, correlations
        )
        
        # Affichage des graphiques
        st.plotly_chart(fig_correlation, use_container_width=True, key="correlation_chart")
        
        col_viz1, col_viz2 = st.columns(2)
        
        with col_viz1:
            st.plotly_chart(fig_heatmap, use_container_width=True, key="heatmap_chart")
        
        with col_viz2:
            # Graphique de r√©partition des priorit√©s (STATIQUE)
            priority_counts = pd.Series([a['priority'] for a in afp_articles]).value_counts()
            fig_priority = px.pie(
                values=priority_counts.values,
                names=priority_counts.index,
                title="üö® R√©partition des Priorit√©s AFP",
                color_discrete_map={'URGENT': '#FF4444', 'FLASH': '#FF8800', 'NORMAL': '#44AA44'}
            )
            st.plotly_chart(fig_priority, use_container_width=True, key="priority_pie")
    
    with tab3:
        st.markdown("### ‚è∞ Analyse Temporelle et Propagation")
        
        st.plotly_chart(fig_timeline, use_container_width=True, key="timeline_chart")
        
        # M√©triques de propagation
        col_t1, col_t2, col_t3 = st.columns(3)
        
        with col_t1:
            # Vitesse de propagation moyenne
            avg_reddit_delay = np.mean([
                (r['timestamp'] - next((a['timestamp'] for a in afp_articles if a['id'] == r['afp_source']), r['timestamp'])).total_seconds() / 3600
                for r in reddit_discussions
            ])
            st.metric(
                "‚ö° D√©lai Reddit Moyen",
                f"{avg_reddit_delay:.1f}h",
                "Temps AFP ‚Üí Reddit",
                help="Temps moyen entre publication AFP et apparition sur Reddit"
            )
        
        with col_t2:
            avg_gdelt_delay = np.mean([
                (g['timestamp'] - next((a['timestamp'] for a in afp_articles if a['id'] == g['afp_source']), g['timestamp'])).total_seconds() / 3600
                for g in gdelt_events
            ])
            st.metric(
                "üåç D√©lai GDELT Moyen",
                f"{avg_gdelt_delay:.1f}h",
                "Temps AFP ‚Üí GDELT",
                help="Temps moyen entre publication AFP et corr√©lation GDELT"
            )
        
        with col_t3:
            propagation_efficiency = (len(reddit_discussions) + len(gdelt_events)) / len(afp_articles)
            st.metric(
                "üìà Efficacit√© Propagation",
                f"{propagation_efficiency:.1f}x",
                "Ratio de diffusion",
                help="Nombre moyen de correspondances par article AFP"
            )
    
    with tab4:
        st.markdown("### üéØ M√©triques Cross-Source Avanc√©es")
        
        # Tableau de performance d√©taill√©
        performance_data = []
        for correlation in correlations:
            article = next((a for a in afp_articles if a['id'] == correlation['afp_id']), None)
            if article:
                performance_data.append({
                    'Article': correlation['afp_title'][:40] + '...',
                    'Cat√©gorie': article['category'].title(),
                    'Priorit√©': article['priority'],
                    'Corr√©lation Reddit': f"{correlation['reddit_correlation']:.1%}",
                    'Corr√©lation GDELT': f"{correlation['gdelt_correlation']:.1%}",
                    'Score Global': f"{correlation['overall_correlation']:.1%}",
                    'Matches Reddit': correlation['reddit_matches'],
                    'Matches GDELT': correlation['gdelt_matches'],
                    'Engagement Total': f"{correlation['total_engagement']:,}"
                })
        
        df_performance = pd.DataFrame(performance_data)
        st.dataframe(
            df_performance,
            use_container_width=True,
            height=400
        )
        
        # Statistiques globales
        st.markdown("#### üìä Statistiques Globales du Syst√®me")
        
        col_s1, col_s2, col_s3, col_s4 = st.columns(4)
        
        with col_s1:
            avg_reliability = np.mean([a['reliability_score'] for a in afp_articles])
            st.metric("üéØ Fiabilit√© AFP Moyenne", f"{avg_reliability:.1%}")
        
        with col_s2:
            avg_reddit_quality = np.mean([r['discussion_metrics']['information_quality'] for r in reddit_discussions])
            st.metric("üí¨ Qualit√© Reddit Moyenne", f"{avg_reddit_quality:.1%}")
        
        with col_s3:
            avg_gdelt_coverage = np.mean([g['coverage_score'] for g in gdelt_events])
            st.metric("üåç Couverture GDELT Moyenne", f"{avg_gdelt_coverage:.1%}")
        
        with col_s4:
            system_health = (avg_reliability + avg_reddit_quality + avg_gdelt_coverage) / 3
            health_status = "üü¢ Excellent" if system_health > 0.8 else "üü° Bon" if system_health > 0.6 else "üî¥ Attention"
            st.metric("‚ù§Ô∏è Sant√© Syst√®me", f"{system_health:.1%}", health_status)
    
    # Footer avec informations syst√®me avanc√©es
    st.markdown("---")
    current_time = datetime.now()
    st.markdown(f"""
    <div style="text-align: center; color: #95A5A6; padding: 25px; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); border-radius: 10px;">
        <p><strong>üîÑ Derni√®re mise √† jour:</strong> {current_time.strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>‚öôÔ∏è Mode actualisation:</strong> {"üü¢ AUTOMATIQUE" if auto_refresh else "üîÑ MANUEL"} 
           {f"(Intervalle: {refresh_interval}s)" if auto_refresh else ""}</p>
        <p><strong>üìä √âtat du syst√®me:</strong> 
           Articles AFP: {len(afp_articles)} | 
           Discussions Reddit: {len(reddit_discussions)} | 
           √âv√©nements GDELT: {len(gdelt_events)} | 
           Corr√©lations: {len(correlations)}</p>
        <p><strong>üéØ Performance:</strong> 
           Corr√©lation moyenne: {np.mean([c['overall_correlation'] for c in correlations]):.1%} | 
           Engagement total: {int(total_weighted_engagement):,} | 
           P√©riode: Derni√®res 24h</p>
        <p><em>üöÄ AFP vs Reddit vs GDELT Analytics v3.0 | Enhanced Real-Time Cross-Source Analysis</em></p>
    </div>
    """, unsafe_allow_html=True)
    
    # Syst√®me d'actualisation automatique optimis√©
    if auto_refresh:
        # Placeholder pour le compteur
        countdown_placeholder = st.empty()
        
        # Compteur visuel
        for i in range(refresh_interval, 0, -1):
            countdown_placeholder.info(f"üîÑ Prochaine actualisation dans {i} secondes... (Mode automatique activ√©)")
            time.sleep(1)
        
        countdown_placeholder.empty()
        st.rerun()

if __name__ == "__main__":
    main()