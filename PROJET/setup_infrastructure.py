# üèóÔ∏è ARCHITECTURE SETUP SCRIPT
# Configuration automatique de l'infrastructure Kafka + Spark
# Compatible Windows PowerShell

import os
import sys
import subprocess
import time
import logging
from pathlib import Path
import requests
import zipfile
import shutil
from typing import Optional, Dict, List

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('setup.log'),
        logging.StreamHandler()
    ]
)

class InfrastructureSetup:
    """Classe pour configurer l'infrastructure Kafka + Spark sur Windows"""
    
    def __init__(self):
        self.logger = logging.getLogger("InfrastructureSetup")
        self.project_dir = Path(__file__).parent.absolute()
        self.downloads_dir = self.project_dir / "downloads"
        self.kafka_dir = None
        self.spark_dir = None
        
        # Versions recommand√©es
        self.kafka_version = "2.13-3.6.0"
        self.spark_version = "3.5.0"
        
        # URLs de t√©l√©chargement (URLs corrig√©es)
        self.kafka_url = f"https://archive.apache.org/dist/kafka/3.6.0/kafka_{self.kafka_version}.tgz"
        self.spark_url = f"https://archive.apache.org/dist/spark/spark-{self.spark_version}/spark-{self.spark_version}-bin-hadoop3.tgz"
        
        # Cr√©er dossier downloads
        self.downloads_dir.mkdir(exist_ok=True)
    
    def check_java(self) -> bool:
        """V√©rifie que Java est install√© et configur√©"""
        try:
            result = subprocess.run(
                ["java", "-version"], 
                capture_output=True, 
                text=True,
                shell=True
            )
            
            if result.returncode == 0:
                java_version = result.stderr.split('\n')[0]
                self.logger.info(f"‚úÖ Java d√©tect√©: {java_version}")
                return True
            else:
                self.logger.error("‚ùå Java non trouv√©")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur v√©rification Java: {e}")
            return False
    
    def install_python_dependencies(self) -> bool:
        """Installe les d√©pendances Python"""
        try:
            self.logger.info("üì¶ Installation des d√©pendances Python...")
            
            # Mise √† jour pip
            subprocess.run([
                sys.executable, "-m", "pip", "install", "--upgrade", "pip"
            ], check=True)
            
            # Installation requirements
            requirements_file = self.project_dir / "requirements.txt"
            if requirements_file.exists():
                subprocess.run([
                    sys.executable, "-m", "pip", "install", "-r", str(requirements_file)
                ], check=True)
                
                self.logger.info("‚úÖ D√©pendances Python install√©es")
                return True
            else:
                self.logger.error("‚ùå Fichier requirements.txt non trouv√©")
                return False
                
        except subprocess.CalledProcessError as e:
            self.logger.error(f"‚ùå Erreur installation d√©pendances: {e}")
            return False
    
    def download_and_extract(self, url: str, filename: str) -> Optional[Path]:
        """T√©l√©charge et extrait une archive"""
        try:
            file_path = self.downloads_dir / filename
            
            # T√©l√©charger si n'existe pas
            if not file_path.exists():
                self.logger.info(f"‚¨áÔ∏è T√©l√©chargement {filename}...")
                
                response = requests.get(url, stream=True)
                response.raise_for_status()
                
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                self.logger.info(f"‚úÖ {filename} t√©l√©charg√©")
            else:
                self.logger.info(f"üìÅ {filename} d√©j√† pr√©sent")
            
            # Extraction
            extract_dir = self.downloads_dir / filename.replace('.tgz', '').replace('.zip', '')
            
            if not extract_dir.exists():
                self.logger.info(f"üìÇ Extraction {filename}...")
                
                if filename.endswith('.tgz'):
                    import tarfile
                    with tarfile.open(file_path, 'r:gz') as tar:
                        tar.extractall(self.downloads_dir)
                elif filename.endswith('.zip'):
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        zip_ref.extractall(self.downloads_dir)
                
                self.logger.info(f"‚úÖ {filename} extrait")
            
            # Trouver le dossier extrait
            for item in self.downloads_dir.iterdir():
                if item.is_dir() and filename.replace('.tgz', '').replace('.zip', '') in item.name:
                    return item
            
            return extract_dir
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur t√©l√©chargement/extraction {filename}: {e}")
            return None
    
    def setup_kafka(self) -> bool:
        """Configure Apache Kafka"""
        try:
            self.logger.info("üîß Configuration Apache Kafka...")
            
            # T√©l√©charger Kafka
            kafka_filename = f"kafka_{self.kafka_version}.tgz"
            self.kafka_dir = self.download_and_extract(self.kafka_url, kafka_filename)
            
            if not self.kafka_dir:
                return False
            
            # Cr√©er scripts de d√©marrage Windows
            self.create_kafka_scripts()
            
            self.logger.info("‚úÖ Apache Kafka configur√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur configuration Kafka: {e}")
            return False
    
    def create_kafka_scripts(self):
        """Cr√©e les scripts de d√©marrage Kafka pour Windows"""
        scripts_dir = self.project_dir / "scripts"
        scripts_dir.mkdir(exist_ok=True)
        
        # Script d√©marrage Zookeeper
        zookeeper_script = scripts_dir / "start_zookeeper.bat"
        zookeeper_content = f"""@echo off
echo Starting Zookeeper...
cd /d "{self.kafka_dir}"
bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties
"""
        with open(zookeeper_script, 'w') as f:
            f.write(zookeeper_content)
        
        # Script d√©marrage Kafka
        kafka_script = scripts_dir / "start_kafka.bat"
        kafka_content = f"""@echo off
echo Starting Kafka Server...
cd /d "{self.kafka_dir}"
bin\\windows\\kafka-server-start.bat config\\server.properties
"""
        with open(kafka_script, 'w') as f:
            f.write(kafka_content)
        
        # Script cr√©ation topics
        topics_script = scripts_dir / "create_topics.bat"
        topics_content = f"""@echo off
echo Creating Kafka Topics...
cd /d "{self.kafka_dir}"

rem Topic pour Reddit
bin\\windows\\kafka-topics.bat --create --topic reddit_stream --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

rem Topic pour Twitter  
bin\\windows\\kafka-topics.bat --create --topic twitter_stream --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

rem Topic pour IoT Sensors
bin\\windows\\kafka-topics.bat --create --topic iot_sensors --bootstrap-server localhost:9092 --partitions 5 --replication-factor 1

rem Topic pour News Feed
bin\\windows\\kafka-topics.bat --create --topic news_feed --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1

echo Topics created successfully!
pause
"""
        with open(topics_script, 'w') as f:
            f.write(topics_content)
        
        self.logger.info("‚úÖ Scripts Kafka cr√©√©s")
    
    def setup_spark(self) -> bool:
        """Configure Apache Spark"""
        try:
            self.logger.info("‚ö° Configuration Apache Spark...")
            
            # T√©l√©charger Spark
            spark_filename = f"spark-{self.spark_version}-bin-hadoop3.tgz"
            self.spark_dir = self.download_and_extract(self.spark_url, spark_filename)
            
            if not self.spark_dir:
                return False
            
            # Configurer variables d'environnement
            self.setup_spark_environment()
            
            # Cr√©er scripts de d√©marrage
            self.create_spark_scripts()
            
            self.logger.info("‚úÖ Apache Spark configur√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur configuration Spark: {e}")
            return False
    
    def setup_spark_environment(self):
        """Configure les variables d'environnement Spark"""
        # Cr√©er script d'environnement
        env_script = self.project_dir / "spark_env.bat"
        env_content = f"""@echo off
set SPARK_HOME={self.spark_dir}
set PYSPARK_PYTHON={sys.executable}
set PYSPARK_DRIVER_PYTHON={sys.executable}
set PATH=%SPARK_HOME%\\bin;%PATH%

echo Spark Environment configured
echo SPARK_HOME: %SPARK_HOME%
echo PYSPARK_PYTHON: %PYSPARK_PYTHON%
"""
        with open(env_script, 'w') as f:
            f.write(env_content)
        
        self.logger.info("‚úÖ Variables environnement Spark configur√©es")
    
    def create_spark_scripts(self):
        """Cr√©e les scripts de d√©marrage Spark"""
        scripts_dir = self.project_dir / "scripts"
        scripts_dir.mkdir(exist_ok=True)
        
        # Script d√©marrage Spark Master
        master_script = scripts_dir / "start_spark_master.bat"
        master_content = f"""@echo off
call "{self.project_dir}\\spark_env.bat"
echo Starting Spark Master...
cd /d "%SPARK_HOME%"
bin\\spark-class.cmd org.apache.spark.deploy.master.Master
"""
        with open(master_script, 'w') as f:
            f.write(master_content)
        
        # Script d√©marrage Spark Worker
        worker_script = scripts_dir / "start_spark_worker.bat"
        worker_content = f"""@echo off
call "{self.project_dir}\\spark_env.bat"
echo Starting Spark Worker...
cd /d "%SPARK_HOME%"
bin\\spark-class.cmd org.apache.spark.deploy.worker.Worker spark://localhost:7077
"""
        with open(worker_script, 'w') as f:
            f.write(worker_content)
        
        self.logger.info("‚úÖ Scripts Spark cr√©√©s")
    
    def setup_mongodb(self) -> bool:
        """Instructions pour MongoDB"""
        self.logger.info("üìÑ Instructions MongoDB:")
        self.logger.info("1. T√©l√©charger MongoDB Community Server")
        self.logger.info("2. Installer avec configuration par d√©faut")
        self.logger.info("3. D√©marrer service MongoDB")
        self.logger.info("4. MongoDB sera accessible sur mongodb://localhost:27017")
        return True
    
    def create_launch_script(self):
        """Cr√©e le script de lancement principal"""
        launch_script = self.project_dir / "launch_system.bat"
        launch_content = f"""@echo off
echo ========================================
echo   Multi-Source Analytics System
echo   Apache Kafka + Spark Streaming
echo ========================================
echo.

echo Etape 1: Demarrage Zookeeper...
start "Zookeeper" cmd /k "scripts\\start_zookeeper.bat"
timeout /t 10

echo Etape 2: Demarrage Kafka...
start "Kafka" cmd /k "scripts\\start_kafka.bat"
timeout /t 15

echo Etape 3: Creation des topics...
call scripts\\create_topics.bat

echo Etape 4: Demarrage Spark Master...
start "Spark Master" cmd /k "scripts\\start_spark_master.bat"
timeout /t 10

echo Etape 5: Demarrage Spark Worker...
start "Spark Worker" cmd /k "scripts\\start_spark_worker.bat"
timeout /t 5

echo.
echo ========================================
echo   Systeme demarre avec succes!
echo ========================================
echo.
echo Services disponibles:
echo - Kafka: localhost:9092
echo - Spark Master: localhost:7077
echo - Spark UI: http://localhost:8080
echo - MongoDB: localhost:27017
echo.
echo Pour demarrer les producteurs:
echo   python kafka_producers.py
echo.
echo Pour demarrer le consumer Spark:
echo   python spark_streaming_consumer.py
echo.
echo Pour le dashboard:
echo   streamlit run dashboard_3d_realtime.py
echo.
pause
"""
        with open(launch_script, 'w', encoding='utf-8') as f:
            f.write(launch_content)
        
        self.logger.info("‚úÖ Script de lancement cr√©√©")
    
    def create_readme(self):
        """Cr√©e la documentation"""
        readme_content = """# üöÄ Multi-Source Real-Time Analytics System

## Architecture
- **Apache Kafka**: Message streaming platform
- **Apache Spark**: Real-time stream processing
- **MongoDB**: NoSQL storage for analytics
- **Streamlit**: Interactive 3D dashboard

## üèóÔ∏è Installation Automatique

```bash
python setup_infrastructure.py
```

## üöÄ D√©marrage Rapide

### Option 1: Script automatique
```bash
./launch_system.bat
```

### Option 2: D√©marrage manuel

1. **D√©marrer Zookeeper**
```bash
./scripts/start_zookeeper.bat
```

2. **D√©marrer Kafka**
```bash
./scripts/start_kafka.bat
```

3. **Cr√©er les topics**
```bash
./scripts/create_topics.bat
```

4. **D√©marrer Spark**
```bash
./scripts/start_spark_master.bat
./scripts/start_spark_worker.bat
```

5. **Lancer les producteurs**
```bash
python kafka_producers.py
```

6. **Lancer le consumer Spark**
```bash
python spark_streaming_consumer.py
```

7. **Ouvrir le dashboard**
```bash
streamlit run dashboard_3d_realtime.py
```

## üìä Dashboard Features

- **Visualizations 3D** des trending keywords
- **Surface plots** pour √©volution sentiment
- **D√©tection d'anomalies** en temps r√©el
- **Analytics cross-platform** sans red√©marrage
- **Monitoring multi-sources** (Reddit, Twitter, IoT, News)

## üîß Configuration

### Kafka Topics
- `reddit_stream`: Donn√©es Reddit (3 partitions)
- `twitter_stream`: Donn√©es Twitter (3 partitions)  
- `iot_sensors`: Capteurs IoT (5 partitions)
- `news_feed`: Flux d'actualit√©s (2 partitions)

### Services URLs
- Kafka: `localhost:9092`
- Spark UI: `http://localhost:8080`
- MongoDB: `mongodb://localhost:27017`
- Dashboard: `http://localhost:8501`

## üìà Analytics Capabilities

- **Sentiment Analysis**: VADER + TextBlob + Lexicon-based
- **Keyword Trending**: Fen√™tres glissantes avec scoring
- **Anomaly Detection**: Z-score et seuils statistiques
- **Cross-Source Correlation**: Analyse inter-plateformes
- **Real-Time Visualization**: Mises √† jour sans interruption

## üéØ Final Project Compliance

‚úÖ **Multi-Source Data Ingestion**: Reddit, Twitter, IoT, News  
‚úÖ **Apache Kafka**: Message streaming  
‚úÖ **Spark Streaming**: Real-time processing  
‚úÖ **Text Analytics**: NLP et sentiment analysis  
‚úÖ **NoSQL Storage**: MongoDB int√©gration  
‚úÖ **3D Visualizations**: Plotly 3D plots  
‚úÖ **Real-Time Monitoring**: Dashboard temps r√©el  

## üîß Troubleshooting

### Java non trouv√©
```bash
# Installer Java 8 ou 11
# Configurer JAVA_HOME
```

### Erreurs Kafka
```bash
# V√©rifier Zookeeper actif
# Ports 9092, 2181 libres
```

### Erreurs Spark
```bash
# V√©rifier SPARK_HOME
# Python et PySpark compatibles
```
"""
        
        readme_file = self.project_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        self.logger.info("‚úÖ Documentation cr√©√©e")

def main():
    """Fonction principale d'installation"""
    print("üöÄ INSTALLATION INFRASTRUCTURE MULTI-SOURCE ANALYTICS")
    print("=" * 60)
    print("Configuration automatique:")
    print("‚Ä¢ ‚òï V√©rification Java")
    print("‚Ä¢ üì¶ Installation d√©pendances Python")
    print("‚Ä¢ üîß Configuration Apache Kafka")
    print("‚Ä¢ ‚ö° Configuration Apache Spark")
    print("‚Ä¢ üìÑ Cr√©ation scripts de d√©marrage")
    print("‚Ä¢ üìö G√©n√©ration documentation")
    print()
    
    setup = InfrastructureSetup()
    
    # √âtapes d'installation
    steps = [
        ("‚òï V√©rification Java", setup.check_java),
        ("üì¶ Installation d√©pendances Python", setup.install_python_dependencies),
        ("üîß Configuration Apache Kafka", setup.setup_kafka),
        ("‚ö° Configuration Apache Spark", setup.setup_spark),
        ("üíæ Instructions MongoDB", setup.setup_mongodb),
        ("üöÄ Cr√©ation script de lancement", setup.create_launch_script),
        ("üìö G√©n√©ration documentation", setup.create_readme)
    ]
    
    success_count = 0
    
    for step_name, step_func in steps:
        try:
            print(f"\n{step_name}...")
            if step_func():
                print(f"‚úÖ {step_name} - SUCCESS")
                success_count += 1
            else:
                print(f"‚ùå {step_name} - FAILED")
        except Exception as e:
            print(f"‚ùå {step_name} - ERROR: {e}")
    
    print("\n" + "=" * 60)
    print(f"üìä INSTALLATION TERMIN√âE: {success_count}/{len(steps)} √©tapes r√©ussies")
    
    if success_count == len(steps):
        print("‚úÖ Installation compl√®te avec succ√®s!")
        print("\nüöÄ PROCHAINES √âTAPES:")
        print("1. D√©marrer le syst√®me: ./launch_system.bat")
        print("2. Ou suivre les instructions dans README.md")
        print("3. Acc√©der au dashboard: http://localhost:8501")
    else:
        print("‚ö†Ô∏è Installation partiellement r√©ussie")
        print("Consulter setup.log pour plus de d√©tails")
    
    print("\nüìö Documentation compl√®te disponible dans README.md")

if __name__ == "__main__":
    main()